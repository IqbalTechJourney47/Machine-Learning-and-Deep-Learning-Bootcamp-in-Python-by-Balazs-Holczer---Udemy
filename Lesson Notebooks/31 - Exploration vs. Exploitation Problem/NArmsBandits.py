from numpy.random import random
from random import randint

# epsilon greedy strategy the agent visit new states with epsilon probability
EPSILON = 0.1
# number of bandits
BANDITS = 3
# number of iterations
EPISODES = 10000

############################################################################################
# EPSILON = 0.1, so this epsilon parameter is going to define the trade off between exploration and exploitation.

# So the agent will explore (it means that it is going to choose a random bandit with epsilon ε probability, and
# it is going to exploit with 1-ε probability, which means that it is going to choose the actual best bandit with
# 1-ε probabilty.)

# So with the help of ε parameter, we are able to tune our algorithm between exploration and exploitation.

# 0.1 means 10% (it means the artificial agent visit new states with 10% probability, and it is going to exploit
# the known states with 90% probability.)

# This is the exploration, exploitation trade off.

############################################################################################
# BANDITS = 3, it means that the number of bandits is equal to 3.
# EPISODES / iterations = 10000
############################################################################################

class Bandit:

    def __init__(self, probability):
        # Qk(a) stores the mean of rewards
        self.q = 0
        # k means how many times action a (so the bandit) was chosen in the past
        self.k = 0
        # probability distribution
        self.probability = probability

# We have to create the constructor or the initialization block with a given probability.
# We are going to assign a probability to every single and armed bandit def __init__(self, probability)

# self.q = 0, and we have the Q value initialized to 0 that stores the mean of the reward. This is the Q function value.

# self.k = 0, we have k initialized to 0.
# So how many times action a so the bandit was chosen in the past.

# The Q values are Q sub k values. The a refers to a given bandit.
# The k means how many times action a so the given bandit was chosen in the past.

# We are able to end up with a recursive formula.

########################################################
# Recursive formula , Q sub k+1 (a) = Q sub k (a) + 1 / k+1 [r sub k+1 - Q sub k (a)]
########################################################

# We just have to know the k parameter in order to update the Q parameter
# and this k parameter is the expected long-term reward of taking action a
# so basically choosing the given bandit

# Q(s,a) expected long-term reward of taking action a when the agent is in state s

# probability distribution
# self.probability = probability

# First bandit has 70% success rate, which means there is 70% chance that the reward is +1, if the agent uses this bandit
# Second bandit has 30% success rate, which means there is 30% chance that the reward is +1, if the agent uses this bandit
# Third bandit has 50% success rate, which means there is 50% chance that the reward is +1, if the agent uses this bandit

# Then we have to define a get-reward function associated with a given bandit

def get_reward(self):
    # rewards can be +1 (win) or 0 (lose)
    if random() < self.probability:
        return 1
    else:
        return 0

# So this is the reward that can be +1 or 0,
# +1 means win situation, 0 means lose situation

# So we generate a random value within the range 0 and 1

# and if it is smaller than self probability, so the probability that is assigned to the given bandit
# then we return 1, which means that it is a win situation
# Otherwise, we return 0, that means it is a lose situation

class NArmedBandit:

    def __init__(self):
        self.bandits = []
        self.bandits.append(Bandit(0.5))
        self.bandits.append(Bandit(0.6))
        self.bandits.append(Bandit(0.4))

# Bandit(0.5)
# We can win with 50% probability, and we can lose with 50% probability.
# Bandit(0.6)
# We can win with 60% probability, and we can lose with 40% probability.
# Bandit(0.4)
# The probability of winning is 40%, and the probability of losing is 60%.

def run(self):
    for i in range(EPISODES):
        bandit = self.bandits[self.select_bandit()]
        reward = bandit.get_reward()
        self.update(bandit, reward)
        print('Iteration %s, bandit %s with Q value %s' % (i, bandit.probability, bandit.q))

# run method is going to create as many iterations as the value of the episodes(EPISODES) variable.
# It is initialized to 10000.

# bandit = self.bandits[self.select_bandit()]
# We select a given bandit based on the ε greedy strategy

# reward = bandit.get_reward()
# We get the reward of the given bandit

# self.update(bandit, reward)
# We update the given bandit and the given reward

# print('Iteration %s, bandit %s with Q value %s' % (i, bandit.probability, bandit.q))
# we print out that
# at the given iteration , Iteration %s
# with the given bandit, bandit %s
# and the Q value is equal to, with Q value %s
# bandit q value, bandit.q

# print('Iteration %s, bandit %s with Q value %s' % (i, bandit.probability, bandit.q))
# So we are going to identify a given bandit based on the probability, bandit.probability
# we have 0.5, 0.6, 0.4 probabilities

############################################################################################
# bandit = self.bandits[self.select_bandit()]

# Then, we have the select_bandit() function

def select_bandit(self):
    # this is the epsilon-greedy strategy
    # with epsilon probability the agent explore - otherwise it exploits
    if random() < EPSILON:
        bandit_index = randint(0, BANDITS - 1)
    else:
        bandit_index = self.get_bandit_max_q()

    return bandit_index

# The select_bandit function is going to return with an index of the given bandit
# 0 - if we want to get the first bandit , Bandit(0.5), index 0
# 1 - if we want to get the first bandit , Bandit(0.6), index 1
# 2 - if we want to get the first bandit , Bandit(0.4), index 2

# this is the epsilon-greedy strategy
# with epsilon probability the agent explore - otherwise it exploits
# So this is the exploration exploitation problem

#     if random() < EPSILON:
# So if we generate a random number within the range 0 and 1, and if it is smaller than EPSILON
# EPSILON = 0.1
# If the given random value is smaller than 10%, then we are going to visit new states

#         bandit_index = randint(0, BANDITS - 1)
# which means that we are going to choose a given bandit at random

# So this is why we generate an index within the range 0, and the number of BANDITS - 1
# This is how we choose a bandit at random, this is how we explore new states. This is exploration.

#    else:
#        bandit_index = self.get_bandit_max_q()
# otherwise, we have exploitation.
# So we will use the bandit with the maximum Q value.
# EPSILON = 0.1
# So with 90% probability
# As EPSILON = 0.1, with 10% probability, we are going to generate random bandits
# It means with 90% probability, we are going to exploit the bandit with the maximum Q value.

# Then we have an update function

def update(self, bandit, reward):
    bandit.k = bandit.k + 1
    bandit.q = bandit.q + (1 / (1 + bandit.k)) * (reward - bandit.q)

# In the update function, we have the bandit and the reward.

########################################################
# Recursive formula , Q sub k+1 (a) = Q sub k (a) + 1 / k+1 [r sub k+1 - Q sub k (a)]
########################################################

# We just have to use this recursive formula

# It is advisable to use this formula because we are able to update the Q value and set Q sub k+1
# with the half of the actual value of Q

# We can see if we know Q sub k, basically we know how to update the next Q value

#    bandit.k = bandit.k + 1
# It increments k + 1 because

#class Bandit:

#    def __init__(self, probability):
        # Qk(a) stores the mean of rewards
#        self.q = 0
        # k means how many times action a (so the bandit) was chosen in the past
#        self.k = 0
        # probability distribution
#        self.probability = probability

# self.k = 0
# k variable counts how many times the given bandit has been considered

def get_bandit_max_q(self):
    # we find the bandit with max Q(a) value for the greedy exploitation
    # we need the index of the bandit with max Q(a)
    max_q_bandit_index = 0
    max_q = self.bandits[max_q_bandit_index.q]

    for i in range(1, BANDITS):
        if self.bandits[i].q > max_q:
            max_q = self.bandits[i].q
            max_q_bandit_index = i

    return max_q_bandit_index

# get_bandit_max_q()
# We have a function in order to get to the index of the bandit with the maximum q value (max_q_bandit_index)

# So we find the bandit with max Q(a) value for the greedy exploitation

# So this is why we need the index of the bandit with max Q(a) value

# max_q_bandit_index = 0
# So first we have the index

#    max_q = self.bandits[max_q_bandit_index.q]
# We get the given bandit associated with the given index and we get the Q value

#    for i in range(1, BANDITS):
#        if self.bandits[i].q > max_q:
#            max_q = self.bandits[i].q
#            max_q_bandit_index = i

# self.bandits[i].q - Q value
# We consider all the bandits and if Q value is larger than the max_q value

# max_q = self.bandits[i].q
# then we update the max_q

# max_q_bandit_index = i
# and the index of the given bandit

# This is a typical maximum finding algorithm

# We consider all of the bandits and we find the bandit with maximum Q value, and we track the index of that given item

#def select_bandit(self):
    # this is the epsilon-greedy strategy
    # with epsilon probability the agent explore - otherwise it exploits
#    if random() < EPSILON:
#        bandit_index = randint(0, BANDITS - 1)
#    else:
#        bandit_index = self.get_bandit_max_q()

#    return bandit_index

# It is crucial during exploitation
# because if we want to select a bandit [select_bandit()] based on the epsilon greedy strategy,
# then sometimes we have to get the bandit with the maximum Q index [self.get_bandit_max_q()]

# This is why we need the given function

#def get_bandit_max_q(self):
    # we find the bandit with max Q(a) value for the greedy exploitation
    # we need the index of the bandit with max Q(a)
#    max_q_bandit_index = 0
#    max_q = self.bandits[max_q_bandit_index.q]

#    for i in range(1, BANDITS):
#        if self.bandits[i].q > max_q:
#            max_q = self.bandits[i].q
#            max_q_bandit_index = i

#    return max_q_bandit_index

#def run(self):
#    for i in range(EPISODES):
#        bandit = self.bandits[self.select_bandit()]
#        reward = bandit.get_reward()
#        self.update(bandit, reward)
#        print('Iteration %s, bandit %s with Q value %s' % (i, bandit.probability, bandit.q))

#    def __init__(self):
#        self.bandits = []
#        self.bandits.append(Bandit(0.5))
#        self.bandits.append(Bandit(0.6))
#        self.bandits.append(Bandit(0.4))

# So with 90% probability, we will select the bandit [self.select_bandit()] with the highest probability [Bandit(0.6)]

# and with 10% probability, we are going to explore the environment,
# which means that we are going to select out bandits at random [Bandit(0.5), Bandit(0.6), Bandit(0.4)]

# show_statistics: how many times the given bandit was chosen
def show_statistics(self):
    for i in range(BANDITS):
        print('Bandit %s with k: %s' % (i, self.bandits[i].k))

#    for i in range(BANDITS):
# so we iterate through all the bandits one by one

#        print('Bandit %s with k: %s' % (i, self.bandits[i].k))
# We print out the index associated with that given bandit,
# and we print out that how many times the given bandit has been considered during the simulation

if __name__ == '__main__':
    bandit_problem = NArmedBandit()
    bandit_problem.run()
    bandit_problem.show_statistics()

# instantiating an armed bandit class [bandit_problem = NArmedBandit()]

# as far as the run function is concerned, we are going to make as many iterations as we have defined
# number of iterations
# EPISODES = 10000

# Conclusion:
# When we run the application, then after 10000 iterations,
# we can come to the conclusion that the epsilon greedy strategy is working fine

# We have 3 bandits in this case

# Bandit with index 1 has the highest probability of winning
# Bandit 0 with k: 511
# Bandit 1 with k: 9137
# Bandit 2 with k: 352

# The algorithm figured out that if we want to maximize the reward, then we have to use the middle bandit [Bandit(0.6)]

# We have some exploration, so the algorithm had come to the conclusion
# that these bandits have a lower probability of winning [Bandit(0.5), Bandit(0.4)]

# So this is the NArmedBandit problem
# So we can solve this with the help of epsilon greedy algorithm

# NOTE:
# This is what we can use during the implementation of artificial intelligence algorithms
# in order to make sure that they are going to explore the environment,
# but on the other hand, they are going to maximize the reward.
# Bandit(0.6)
# Bandit 1 with k: 9137
